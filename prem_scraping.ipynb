{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping Premier League Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aba347a12d7355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef4c5c9aee790d7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data URL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32ffd82df363181b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the url for data\n",
    "prem_standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d6d4003fa44006d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get Data for Each Team and Previous Years"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275dfefbb5981fe4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create year range\n",
    "years = list(range(2024, 2022, -1))\n",
    "\n",
    "#initialize every match list\n",
    "prem_team_matches = []\n",
    "\n",
    "# loop scrape\n",
    "for year in years:\n",
    "\n",
    "    data = requests.get(prem_standings_url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    time.sleep(10) \n",
    "    \n",
    "    # filtering\n",
    "    prem_standings_table = soup.select('table.stats_table')[0]\n",
    "    team_links = [l.get(\"href\") for l in prem_standings_table.find_all('a')]\n",
    "    team_links = [l for l in team_links if '/squads/' in l]\n",
    "    \n",
    "    # get urls\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in team_links]\n",
    "    \n",
    "    # declare previous season url\n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "    standings_url = f\"https://fbref.com{previous_season}\"\n",
    "    \n",
    "    # scrape each individual team\n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        \n",
    "        # use pandas to extract the teams' match data\n",
    "        team_data = requests.get(team_url)\n",
    "        matches = pd.read_html(team_data.text, match=\"Scores & Fixtures\")\n",
    "        \n",
    "        # prevent fbref website slowing down using sleep\n",
    "        time.sleep(10) \n",
    "        \n",
    "        # shooting data\n",
    "        temp_soup = BeautifulSoup(team_data.text)\n",
    "        \n",
    "        # filter\n",
    "        temp_links = [l.get(\"href\") for l in temp_soup.find_all('a')]\n",
    "        temp_links = [l for l in temp_links if 'all_comps/shooting/' in l]\n",
    "\n",
    "        # get shooting data\n",
    "        shooting_data = requests.get(f\"https://fbref.com{temp_links[0]}\")\n",
    "        shooting = pd.read_html(shooting_data.text, match=\"Shooting\")[0]\n",
    "        \n",
    "        # drop first unneeded level\n",
    "        shooting.columns = shooting.columns.droplevel()\n",
    "        \n",
    "        try:\n",
    "            # merge both dataframes\n",
    "            team_data = matches[0].merge(shooting[[\"Date\", \"Sh\",\n",
    "                                                   \"SoT\", \"Dist\",\n",
    "                                                   \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        # filter just premier league\n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "        prem_team_matches.append(team_data)\n",
    "        time.sleep(10) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc5514adb3fefcc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combine All Dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22ee5742ac097fcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use pandas concatenation \n",
    "match_df = pd.concat(prem_team_matches)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a6be11d8329af1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make All Columns Lowercase"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fab9a5fc7129e4d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make columns lowercase\n",
    "match_df.columns = [c.lower() for c in match_df.columns]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164b9bb4c30c65c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Export Data as CSV File for Prediction Part of Project"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d2f6224c0bbd6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# export as csv\n",
    "match_df.to_csv(\"prem_matches.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d741b4c9f1e5157f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
